{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Low-Rank Approximation for Movie Recommendations\n",
    "\n",
    "A common application of low-rank approximation is for recommender systems.  In this lab, we will create a very primitive recommendation system for movies.  Through the lab, you will learn to:\n",
    "\n",
    "* Represent ratings data as a sparse matrix\n",
    "* Evaluate the mean absolute error (MAE) using simple movie or user biases\n",
    "* Create a low-rank model in Tensorflow for predicting the movie rating using `Embedding` layers\n",
    "* Train the model and optimize the embedding dimension.\n",
    "* Make predictions on new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MovieLens Dataset\n",
    "\n",
    "We first load some common packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GroupLens](https://grouplens.org/) is a research organization at the University of Minnesota that has done extensive work in recommendation systems among other topics.  They have excellent datasets on movie recommendations as part of their [MovieLens project](https://movielens.org/).  In this lab, we will use a relatively small dataset, `Movielens 1m` that has 1 million ratings.  If you are interested in continuing research in this area, they have much larger datasets as well.  But, this relatively small one is sufficient to illustrate the basic concepts.\n",
    "\n",
    "To get the data, go to the webpage:\n",
    "\n",
    "https://grouplens.org/datasets/movielens/latest/\n",
    "\n",
    "and download and unzip the files, `ml-1m.zip`.  Alternatively, you can just run the following command which will download unzip the file for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading ml-1m.zip\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-946c7c0b108b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msrc_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mdst_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ml-1m.zip'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mdownload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# Then, unzip the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-946c7c0b108b>\u001b[0m in \u001b[0;36mdownload_file\u001b[1;34m(src_url, dst_fn)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mwrote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         with tqdm.notebook.tqdm(total=total_size//block_size, unit='kB', \n\u001b[0m\u001b[0;32m     28\u001b[0m                            unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         self.container = self.status_printer(\n\u001b[0m\u001b[0;32m    208\u001b[0m             self.fp, total, self.desc, self.ncols)\n\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# Prepare IPython progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# #187 #451 #558 #872\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ImportError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                 \u001b[1;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Set the files names for movies and ratings files\n",
    "ml_dir = 'ml-1m'\n",
    "ratings_fn = os.path.join(ml_dir,'ratings.dat')\n",
    "movies_fn = os.path.join(ml_dir,'movies.dat')\n",
    "\n",
    "def download_file(src_url, dst_fn):\n",
    "    \n",
    "    if os.path.exists(dst_fn):\n",
    "        print('File %s already exists' % dst_fn)\n",
    "        return\n",
    "    \n",
    "    print('Downloading %s' % dst_fn)\n",
    "    \n",
    "    # Streaming, so we can iterate over the response.\n",
    "    r = requests.get(src_url, stream=True)\n",
    "\n",
    "    # Total size in MB.\n",
    "    total_size = int(r.headers.get('content-length', 0)); \n",
    "    block_size = 1024\n",
    "    wrote = 0 \n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        with tqdm.notebook.tqdm(total=total_size//block_size, unit='kB', \n",
    "                           unit_scale=True, unit_divisor=1024) as pbar:\n",
    "            for data in r.iter_content(block_size):\n",
    "                wrote = wrote + len(data)\n",
    "                pbar.update(1)\n",
    "                f.write(data)\n",
    "    if total_size != 0 and wrote != total_size:\n",
    "        print(\"ERROR, something went wrong\") \n",
    "\n",
    "# Test if all files are downloaded\n",
    "files_exists = False\n",
    "if os.path.exists(ml_dir):\n",
    "    if os.path.exists(ratings_fn) and os.path.exists(movies_fn):\n",
    "        files_exists = True\n",
    "\n",
    "if files_exists:\n",
    "    print('Files %s and %s already downloaded' % (ratings_fn, movies_fn))\n",
    "\n",
    "else:\n",
    "    # First download the zip file if needed\n",
    "    src_url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "    dst_fn = 'ml-1m.zip'\n",
    "    download_file(src_url, dst_fn)\n",
    "    \n",
    "    # Then, unzip the file\n",
    "    print('Unzipping %s...' % dst_fn)\n",
    "    zip_ref = zipfile.ZipFile(dst_fn, 'r')\n",
    "    zip_ref.extractall('.')\n",
    "    zip_ref.close()\n",
    "    print('Unzip completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the movies files with the `read_csv` command.  Print the first 5 entries of the dataframe.  You will see that the file has a list of movies.  Each movie has a `movieId` and `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11649\n"
     ]
    }
   ],
   "source": [
    "# TODO:  Read the movies \n",
    "import os\n",
    "dir = './ml-1m/'\n",
    "movies = pd.read_csv('./ml-1m/movies.dat',sep='::', names=['MovieID', 'Title', 'Genres'])\n",
    "print(movies.size)\n",
    "# TODO:  Use the movies.head() to display the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the following columns from the `movies` dataframe:\n",
    "*  Extract the `movieId` column, convert to an `np.array` and store in `movie_ids`\n",
    "*  Extract the `title` column, convert to a list (using `.tolist()`) and store in `titles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "movie_ids = movies['MovieID']\n",
    "titles = movies['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function returns the string of a movie title, given its movie id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_title(movie_id):\n",
    "    I = np.where(movie_ids == movie_id)[0]\n",
    "    if len(I) == 0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return titles[I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `ratings.dat` file into a `pandas` dataframe `ratings`.  Use the `head` method to print the first five rows of the dataframe.  This is a large file, so it may take a minute to read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserID</th>\n      <th>MovieID</th>\n      <th>Rating</th>\n      <th>Timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1193</td>\n      <td>5</td>\n      <td>978300760</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>661</td>\n      <td>3</td>\n      <td>978302109</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>914</td>\n      <td>3</td>\n      <td>978301968</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3408</td>\n      <td>4</td>\n      <td>978300275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2355</td>\n      <td>5</td>\n      <td>978824291</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# TODO\n",
    "ratings = pd.read_csv('./ml-1m/ratings.dat',sep='::', names=['UserID','MovieID', 'Rating', 'Timestamp'])\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract three columns from the `ratings` dataframe:\n",
    "\n",
    "* Set `user` to `ratings['userId']`,\n",
    "* Set `movie` to `ratings['movieId']`\n",
    "* Set `y` to `ratings['rating']`\n",
    "\n",
    "Convert to each to an `np.array`.  Print:\n",
    "\n",
    "* Total number of movies (the maximum movie index)\n",
    "* Total number of users\n",
    "* Total number of ratings\n",
    "* The average fraction of movies rated per user\n",
    "\n",
    "You should see that only a small fraction of the movies are rated by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of movies:  3952\nTotal number of users:  6040\nTotal number of ratings:  1000209\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "user = ratings['UserID']\n",
    "movie = ratings['MovieID']\n",
    "y = ratings['Rating']\n",
    "print('Total number of movies: ',movie.max())\n",
    "print('Total number of users: ', user.max())\n",
    "print('Total number of ratings: ', y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to predict the rating `y` from the indices `movie` and `user`.  We need to split the data into training and test.\n",
    "Create training and test data of the form:\n",
    "\n",
    "* Training data:  `Xtr = [usertr, movietr]` and `ytr` for approximately 75% of the samples.\n",
    "* Test data:  `Xts = [userts, moviets]` and `yts` for approximately 25% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "tr_size = int(y.size * 0.75)\n",
    "ts_size = y.size - tr_size\n",
    "\n",
    "usertr = user[0:tr_size]\n",
    "movietr = movie[0:tr_size]\n",
    "userts = user[tr_size + 1:]\n",
    "moviets = movie[tr_size + 1:]\n",
    "\n",
    "Xtr = np.transpose(np.vstack((usertr, movietr)))\n",
    "ytr = y[0:tr_size]\n",
    "Xts = np.transpose(np.vstack((userts, moviets)))\n",
    "yts = y[tr_size + 1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Rating Prediction Based on Average Rating\n",
    "\n",
    "Before we try to perform a complex algorithm for predicting a movie rating, we will first compute some simple statistics to get you familar with the data set.  First, compute the average movie rating across all movies in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "y0 = np.average(ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, find the average rating per movie id. For each movie id, `i` compute `ymean[i]`, the average rating for that movie in the training data set and `ycnt[i]`, the number of ratings the movie had.  If `ycnt[i]==0`, set `ymean[i]=y0`, where `y0` is the average overall rating. \n",
    "\n",
    "You will want to think about how you do this computation efficiently since the data set `ytr` has a large number of entries.  Make you sure you go over the entries in `ytr` only once.  Even if you do it efficiently, it will take a minute, so you may want to add a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3883\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# ymean = ...\n",
    "ymean = [0.0]*movie_ids.max()\n",
    "ycnt = [0]*movie_ids.max()\n",
    "print(movie_ids.size)\n",
    "for i, movieid in enumerate(movietr):\n",
    "    ycnt[movieid-1] += 1\n",
    "    ymean[movieid-1] = (ymean[movieid-1] * (ycnt[movieid-1] - 1) + y[i])/ycnt[movieid-1]\n",
    "for i in range(0, len(ymean)):\n",
    "    if ycnt[i] == 0:\n",
    "        ymean[i] = y0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the movies that had an average rating over 4.8.  Print their titles, the average rating and the number of ratings they had.  You will see that most of the movies with very high ratings had very few ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4.144167758846663, 3.1821631878557874, 3.0600000000000005, 2.6576576576576576, 3.014354066985645]\nMovie  556 :  War Room, The (1993) averate rating:  5.0  number of ratings:  1\nMovie  577 :  Andre (1994) averate rating:  5.0  number of ratings:  1\nMovie  786 :  Eraser (1996) averate rating:  5.0  number of ratings:  3\nMovie  852 :  Tin Cup (1996) averate rating:  5.0  number of ratings:  1\nMovie  988 :  Grace of My Heart (1996) averate rating:  5.0  number of ratings:  1\nMovie  1829 :  Chinese Box (1997) averate rating:  5.0  number of ratings:  1\nMovie  3171 :  Room at the Top (1959) averate rating:  5.0  number of ratings:  1\nMovie  3232 :  Seven Chances (1925) averate rating:  5.0  number of ratings:  2\nMovie  3279 :  Knockout (1999) averate rating:  5.0  number of ratings:  1\nMovie  3880 :  Ballad of Ramblin' Jack, The (2000) averate rating:  5.0  number of ratings:  1\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(ymean[:5])\n",
    "for i, rate_mean in enumerate(ymean):\n",
    "    if rate_mean > 4.8:\n",
    "        print(\"Movie \", i, \": \", get_movie_title(i) ,\"averate rating: \", rate_mean, \" number of ratings: \", ycnt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each `i` in the test data set, compute `yhat[i]` to be the mean rating for the movie in rating `i`.  Find the average value `|yhat[i]-yts[i]|`.  This is called the *mean absolute error* or MAE and is a common metric in evaluating recommendation predictions.  If you did everything correctly, you should get an MAE ~= 0.78.  That means that simply using the average movie rating by users will predict the rating of another user within 0.78 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "yhat = [0] * yts.size\n",
    "for i, movieid in enumerate(moviets):\n",
    "    yhat[i] = ymean[movieid - 1]\n",
    "mae = abs(yhat - yts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network Recommender\n",
    "We now build a neural network for predicting the ratings.  First, we load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense, Flatten, Add, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a neural network in Tensorflow as follows:\n",
    "\n",
    "*  Set the embedding dimension to `emb_dim=4`.  \n",
    "*  Let `userid_in` and `movieid_in` be the input user and movie indices.  These can be created in Tensorflow with `Input` layers with `shape = (1,)`. \n",
    "*  The user index generates a bias `user_bias`.  Use an `Embedding` layer with `output_dim=1` followed by a `Flatten` layer.  \n",
    "*  The user index also generates a weight `user_wt`.  Use a second `Embedding` layer with `output_dim=emb_dim` followed by a `Flatten` layer.  \n",
    "*  The movie index generates biases `movie_bias` and `movie_wt` similar to the user bias.\n",
    "*  We then make the rating prediction with `yhat = Dot(user_wt, movie_wt) + user_bias + movie_bias`. \n",
    "*  Optionally, you can add bias and weight regularization, although I found these did not help significantly.\n",
    "*  Set the model to `mod = Model([userid_in, movieid_in], yhat)`.\n",
    "\n",
    "Print a summary of the model `mod.summary()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nuserid_in (InputLayer)          [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nmovieid_in (InputLayer)         [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuseremb1 (Embedding)            (None, 1, 4)         3000624     userid_in[0][0]                  \n__________________________________________________________________________________________________\nmovieemb1 (Embedding)           (None, 1, 4)         3000624     movieid_in[0][0]                 \n__________________________________________________________________________________________________\nuser_wt (Flatten)               (None, 4)            0           useremb1[0][0]                   \n__________________________________________________________________________________________________\nmovie_wt (Flatten)              (None, 4)            0           movieemb1[0][0]                  \n__________________________________________________________________________________________________\nuseremb0 (Embedding)            (None, 1, 1)         750156      userid_in[0][0]                  \n__________________________________________________________________________________________________\nproduct (Dot)                   (None, 1)            0           user_wt[0][0]                    \n                                                                 movie_wt[0][0]                   \n__________________________________________________________________________________________________\nuser_bias (Flatten)             (None, 1)            0           useremb0[0][0]                   \n__________________________________________________________________________________________________\nmovieemb0 (Embedding)           (None, 1, 1)         750156      movieid_in[0][0]                 \n__________________________________________________________________________________________________\ntf_op_layer_AddV2 (TensorFlowOp [(None, 1)]          0           product[0][0]                    \n                                                                 user_bias[0][0]                  \n__________________________________________________________________________________________________\nmovie_bias (Flatten)            (None, 1)            0           movieemb0[0][0]                  \n__________________________________________________________________________________________________\ntf_op_layer_AddV2_1 (TensorFlow [(None, 1)]          0           tf_op_layer_AddV2[0][0]          \n                                                                 movie_bias[0][0]                 \n==================================================================================================\nTotal params: 7,501,560\nTrainable params: 7,501,560\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "K.clear_session()\n",
    "l2_reg = 1e-8\n",
    "emb_dim = 4\n",
    "userid_in  = Input(name='userid_in', shape=(1,))\n",
    "movieid_in  = Input(name='movieid_in', shape=(1,))\n",
    "useremb0 = Embedding(input_dim=usertr.size, output_dim=1,name='useremb0',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(userid_in)\n",
    "user_bias = Flatten(name='user_bias')(useremb0)\n",
    "useremb1 = Embedding(input_dim=usertr.size, output_dim=emb_dim,name='useremb1',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(userid_in)\n",
    "user_wt = Flatten(name='user_wt')(useremb1)\n",
    "\n",
    "movieemb0 = Embedding(input_dim=movietr.size, output_dim=1,name='movieemb0',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(movieid_in)\n",
    "movie_bias = Flatten(name='movie_bias')(movieemb0)\n",
    "movieemb1 = Embedding(input_dim=movietr.size, output_dim=emb_dim,name='movieemb1',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(movieid_in)\n",
    "movie_wt = Flatten(name='movie_wt')(movieemb1)\n",
    "\n",
    "yhat = Dot(name='product',axes=1)([user_wt, movie_wt]) + user_bias + movie_bias\n",
    "mod = Model([userid_in, movieid_in], yhat)\n",
    "mod.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with `Adam` optimizer with a learning rate of `0.01` (I found these numbers to work out well).  Use the \n",
    "`'mean_absolute_error'` loss.  Then fit the model with 8 epochs.  Use a batch size of 1000.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "InternalError",
     "evalue": "2 root error(s) found.\n  (0) Internal:  Blas xGEMMStridedBatched launch failed : a.shape=[1000,1,4], b.shape=[1000,4,1], m=1, n=1, k=4, batch_size=1000\n\t [[node functional_1/product/MatMul (defined at <ipython-input-34-32931e3bc2d0>:4) ]]\n\t [[AddN_4/_70]]\n  (1) Internal:  Blas xGEMMStridedBatched launch failed : a.shape=[1000,1,4], b.shape=[1000,4,1], m=1, n=1, k=4, batch_size=1000\n\t [[node functional_1/product/MatMul (defined at <ipython-input-34-32931e3bc2d0>:4) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2028]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-32931e3bc2d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0musertr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmovietr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muserts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmoviets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal:  Blas xGEMMStridedBatched launch failed : a.shape=[1000,1,4], b.shape=[1000,4,1], m=1, n=1, k=4, batch_size=1000\n\t [[node functional_1/product/MatMul (defined at <ipython-input-34-32931e3bc2d0>:4) ]]\n\t [[AddN_4/_70]]\n  (1) Internal:  Blas xGEMMStridedBatched launch failed : a.shape=[1000,1,4], b.shape=[1000,4,1], m=1, n=1, k=4, batch_size=1000\n\t [[node functional_1/product/MatMul (defined at <ipython-input-34-32931e3bc2d0>:4) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2028]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "opt = Adam(lr=0.01)\n",
    "mod.compile(optimizer=opt, loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "hist = mod.fit(usertr, movietr,ytr,epochs=8,batch_size=1000, verbose=0, validation_data=(userts, moviets,yts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the training and test loss as a function of the epochs.  If you did it correctly the final test loss should be around  0.71 and the training loss should be 0.68.  This is a little better than the MAE you get just using the average movie rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions!\n",
    "\n",
    "Select a random user, `user_id`.  Then, for each movie index, use the model to predict the ratings. Set the predictions to `yhat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#    user_id = ...\n",
    "#    yhat = mod.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the names of the movies with the top 10 predicted ratings for the user as well as the average rating that those movies had.  You will see that the network may predict ratings above 5!  We could have avoided this by limiting the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus:  Optimizing the Embedding Dimension\n",
    "\n",
    "You can try to optimize the embedding dimension.  Try different dimensions from 0 to 8.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and test loss as a function of the embedding layer. We see we get a minimum with an embedding dimension around 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}