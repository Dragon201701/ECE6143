{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9a: PCA for Face Recognition\n",
    "    \n",
    "Following the demo for this unit, we will explore further the use of PCA for feature dimension reduction for classification. We will use a 2-layer neural net on the PCA coefficients. We will practice optimizing the classificaiton parameters (the number of PCA components and the number of hidden nodes in the NN classifier). We will furthermore compare this approach with using convolutional neural net on raw images.\n",
    "\n",
    "Through the lab, you will learn to:\n",
    "\n",
    "* Perform PCA on the a face dataset to find the PC components\n",
    "* Evaluate the effect of using different nubmer of principle components for data representation and classification.\n",
    "* Optimize the number of PC coefficients and classifier parameters together to maximize classification accuracy.\n",
    "* Understand the impact of training data size on the feature and classification method selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the flw_people dataset. \n",
    "# Select only those people with at least 100 instances \n",
    "# Reduce the face image size by 0.4\n",
    "\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the face images in a datamatrix X and the labels and corresponding names in a datamatrix y and target_names\n",
    "# Furthermore, determine the number of samples and the image size \n",
    "# Determine the number of different faces (number of classes)\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some sample images to make sure your data load is correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and test set with 50% data for training. \n",
    "# Use \"stratify\" option to make sure the training data and test data have same \n",
    "# proportion of images from different faces\n",
    "# print the number of samples in the training data\n",
    "\n",
    "# TO DO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom PCA on the training data to derive the principle components (PCs) and the PCA coefficients \n",
    "# You can directly use the PCA class in PCA package or use SVD.\n",
    "# Remember that you need to remove the mean from the data first\n",
    "# Also you should rescale the PCs so that the PCA coefficients all have unit variance\n",
    "# Determine the total number of PCs\n",
    "\n",
    "# TO DO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First let us construct a 2-layer neural net classifier that uses npc= 100 PCA coefficients to classify the faces.  Set up your training and testing data to contain npc PCA coefficients using the previously determined principle components. You should directly use matrix multiplication (i.e. projecting original data to the first 100 principle components you found previously) to find the coefficients rather then using the pca.transform( ) method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up and compile a NN model with number of hidden nodes nnode=100 and a output layer, and then fit the model to the training data. Use 'relu' for the activation for the hidden layer and use 'softmax' for the output layer. Using `sparse_categorical_crossentropy` for the loss. Use `accuracy` as the metrics. You can choose to do a small number of epochs (=10) with batch size =100.  Determine the accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to identify the best number of PCs and the best number of hidden nodes in the NN classifer that can achieve the highest validation accuracy. \n",
    "You can set the range of PCs and nubmer of hidden nodes as below.\n",
    "\n",
    "nnodes = [50,100,150,200, 250],\n",
    "npcs = [50,100,150,200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up an array to store accuracy for different nnode and npcs\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the combinations to find the accuracy for each combination\n",
    "# For each possible combination of `nnode` and `npc`, set up and fit the model \n",
    "# using features containing only coefficents corresponding to npc coefficients.\n",
    "\n",
    "# TO DO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the npc and nnode that provides the highest validation accuracy \n",
    "# TO DO \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a contour plot of the accuracy using different nnode and npc combincations\n",
    "# TO DO\n",
    "\n",
    "# plt.contourf ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let us compare the PCA+NN with applying a CNN on the raw image data only. \n",
    "\n",
    "Note that you should scale your image data to between 0 and 1. And you should reshape your training and testing data according to image width and height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data preparation for input to CNN\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a CNN model\n",
    "# You can use 2 conv2D layer, each with kernel size of 5x5, each followed by a pooling layer with strides of 2\n",
    "# For this part, let both conv2D layer generate 16 channels. \n",
    "# The Conv layer should be followed by a flatten layer and two dense layers. \n",
    "# The first dense layer should produce 200 outputs. \n",
    "# The last dense layer is the output layer with n_classes output using 'softmax' activation.\n",
    "# Print model summary to verify it follows the desired structure and compile the model\n",
    "\n",
    "# TO DO \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using batch size=100, epochs = 40\n",
    "# Print the accuracy on the validation set\n",
    "\n",
    "# TO DO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How do the result compared with the PCA+NN method? (If you did right, they should be similar, with PCA+NN being slightly better. If you used more training data (e.g. 75%) and you trained the CNN with more epochs, CNN method may get better). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the above using a small dataset\n",
    "\n",
    "Instead of using 50% of the total data for training, let us assume you have only 10% of the total data for training. Repeat both the PCA+NN and the CNN method, to see which one gives you better results. \n",
    "\n",
    "Note that with only 10% data for training, the range of the npc has to be set to be below the total number of training samples. \n",
    "\n",
    "For the CNN model, because you have small number of training samples, you cannot train a network with a large number of parameters reliably. Instead of producing 16 channels for each of the two conv2D layers, configure the model to produce only 8 channels each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does CNN compare with PCA+NN with the small training set? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
